{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning ~ Classification of NYC Water\n",
    "Author: Connor Finn <br>\n",
    "Date: June 18, 2020 <br>\n",
    "Description: <dir>\n",
    "    This script will perform classification on images of the Greater NYC area in order to distinguish between water and land pixels. The ground truth data was provided by Dr. Narayanaswamy, who detailed polygons of land and water in Google Earth, and stored these regions as json files. To use this notebook, please store these files in known locations in your working environment. <br>\n",
    "\n",
    "    \n",
    "References: To develop this code, I referenced Arvind's classification notebook as well as the Google ee [documentation](https://developers.google.com/earth-engine/classification)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Set UP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize google earth engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=qy9Mv6tnyHFZHJthrH7vsG_tjGASdOjUbTqgICCVxRg&code_challenge_method=S256>https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=qy9Mv6tnyHFZHJthrH7vsG_tjGASdOjUbTqgICCVxRg&code_challenge_method=S256</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you\n",
       "        should paste in the box below</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter verification code: 4/1AE8F4SNfaQrRTchpAHpBxrrvGsraySYFYUfq7HAKuoCX5FpToU14yo\n",
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import geojson\n",
    "import json\n",
    "import pygeoj\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "\n",
    "# Trigger the authentication flow.\n",
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location of interest\n",
    "This region of manhattan is where we are building our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/52cd0b184ccb220c70316b0556f5053a-a932f53ff2acf13651df4b3375ec5d20:getPixels\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nycsite = ee.Geometry.Rectangle([-74.04, 40.69, -73.82, 40.94]);\n",
    "\n",
    "image0 = image_list_nyc[5] #one image from the list of images\n",
    "\n",
    "parameters = {'min': 0.0,\n",
    "              'max': 16000.0,\n",
    "              'dimensions': 768,\n",
    "              'bands': ['B4', 'B3', 'B2']}\n",
    "\n",
    "Image(url = image0.clip(nycsite).getThumbUrl(parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Collect Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a Function used to collect a list of google ee image objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(path_list, row_list, satellite, start_date, end_date, max_cloud_percentage):\n",
    "  # This function will get a list of image objects ~ according to the provided information\n",
    "\n",
    "    # get image collection object\n",
    "    coll = ee.ImageCollection(satellite)\\\n",
    "        .filterDate(start_date, end_date)\\\n",
    "        .filter(ee.Filter.inList('WRS_PATH', path_list))\\\n",
    "        .filter(ee.Filter.inList('WRS_ROW', row_list))\\\n",
    "        .filter(ee.Filter.lt('CLOUD_COVER' , max_cloud_percentage))  # note ~ not less than or equal to\n",
    "\n",
    "    # get image_id's\n",
    "    image_ids = list( map( lambda x : x['id'] , coll.getInfo()['features'] ) ) \n",
    "    \n",
    "    # get image objects\n",
    "    images = list( map( lambda x: ee.Image(x) , image_ids ) )\n",
    "    \n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b Collect the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of images to work with\n",
    "p = [14]\n",
    "r = [32]\n",
    "sat = 'LANDSAT/LC08/C01/T1'\n",
    "sd = '2013-05-01'\n",
    "ed = '2020-05-01'\n",
    "cc= 10\n",
    "image_list_nyc = get_images(p, r, sat, sd, ed, cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Ground Truth Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is code from Arvind, this is used to take the ground truth data he created with google earth, and create dictionaries. <br>\n",
    "\n",
    "I have converted it to a function in order to allow for expansion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a: Function to create featurevector of labeled geolocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This Function is compiled using code provided by Arvind. It is essential that we can \n",
    "Figure out a way to create 2 sets of data points which have no overlap. \n",
    "    1. Training ~ 80% \n",
    "    2. Testing  ~ 20%\n",
    "\"\"\"\n",
    "\n",
    "def get_labeled_data(jsonfiles , classalloc , num_points):\n",
    "    '''\n",
    "    Input:\n",
    "        jsonfiles = list of regions in json format\n",
    "        classalloc = list of integer classifiers \n",
    "                + for now, 1 = land, 0 = water\n",
    "    Output:\n",
    "        fc = feature collection to train on\n",
    "        nycfc = another feature collection  \n",
    "    '''\n",
    "\n",
    "\n",
    "    # Dictionaries to store intermediate objects before we get to features and feature collections.\n",
    "    coords_dict = {}\n",
    "    ee_dict = {}\n",
    "    randomPts_dict = {}\n",
    "    features_dict = {}\n",
    "\n",
    "    # Build the dictionaries\n",
    "    n = 0\n",
    "    for jsonfile in jsonfiles:\n",
    "        jsonfilepath =  jsonfile +'.json'\n",
    "        with open(jsonfilepath) as f:\n",
    "            data = geojson.load(f)\n",
    "\n",
    "        #creating a dictionary of coordinates\n",
    "        coords_dict[jsonfile + 'coords'] = np.array(data['features'][0]['geometry']['coordinates'][0])[:,0:2].tolist()\n",
    "\n",
    "        #creating a polygon from coordinate list\n",
    "        ee_dict[jsonfile + 'ee'] = ee.Geometry.Polygon(coords_dict[jsonfile + 'coords'])\n",
    "\n",
    "        \"\"\"\n",
    "        BIG PROBLEM:\n",
    "            How can we make the random points below into two EXCLUSIVE GROUPS\n",
    "\n",
    "        \"\"\"\n",
    "        randomPoints = ee.FeatureCollection.randomPoints(region=ee_dict[jsonfile + 'ee'],points=num_points)\n",
    "\n",
    "\n",
    "        randomPoints = randomPoints.map(lambda x: x.set({'landcover': classalloc[n]})) #This is to add a property named\n",
    "\n",
    "        randomPts_dict[jsonfile+'Pts'] = randomPoints\n",
    "\n",
    "\n",
    "\n",
    "        #randomPts_dict[jsonfile + 'rdnmPts'] = ee.FeatureCollection.randomPoints(ee_dict[jsonfile + 'ee'], 100)\n",
    "        features_dict[jsonfile + 'feature'] = ee.Feature(ee_dict[jsonfile + 'ee'], {'name': jsonfile, 'landcover': classalloc[n]})\n",
    "\n",
    "        n = n+1\n",
    "    '''\n",
    "    The individual features are combined as shown below to create a feature collection.\n",
    "\n",
    "    You can get some information about the features in the collection using commands as shown below.\n",
    "    '''\n",
    "    nycFC = ee.FeatureCollection(list(features_dict.values()))\n",
    "\n",
    "    nycFCpts = ee.FeatureCollection(list(randomPts_dict.values()))\n",
    "\n",
    "    fc = ee.FeatureCollection([])\n",
    "    for x in randomPts_dict.keys():\n",
    "        fc = fc.merge(randomPts_dict[x])\n",
    "\n",
    "    \n",
    "    return fc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b Collect Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground Truth Data\n",
    "jsonfiles = ['Hudson01', 'Hudson02', 'Bronx01', 'Astoria01'];\n",
    "classalloc = [1, 1, 0, 0];\n",
    "num_points = 10000\n",
    "num_points_test = 2000\n",
    "\n",
    "'''\n",
    "Current problem ~ There easily could be overlap between these two samples. need to do a stratafied sample of sorts\n",
    "'''\n",
    "\n",
    "training_feature_collection = get_labeled_data(jsonfiles, classalloc , num_points )\n",
    "testing_feature_collection  = get_labeled_data(jsonfiles, classalloc , num_points_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Build ML Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a ML Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression_Tree_Classifier():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.classifier = None  # EE Classifier object\n",
    "        self.validated = None   # FeatureCollection object\n",
    "    \n",
    "    def set_classifier(self, maxNodes = None , minLeafPopulation = 1):\n",
    "        '''\n",
    "            Input:\n",
    "                maxNodes: The maximum number of nodes ~ i.e. if maxNodes = 3, the decision tree\n",
    "                          will split your dataset two times. (defaults to no limit)\n",
    "                minLeafPopulation: The minimum number of datapoints in each node:\n",
    "\n",
    "            Output: None ~ initializes self.classifer\n",
    "        '''\n",
    "        \n",
    "        self.classifier = ee.Classifier.smileCart(maxNodes = 2 , minLeafPopulation = 100)\n",
    "    \n",
    "    def train(self, train_coll, image, bands , label ):\n",
    "        '''\n",
    "            Input:\n",
    "                train_coll ~ featureCollection object with labeled data\n",
    "                image: image object you will train on\n",
    "                bands: list of strings ('B1' ect) the bands we are training on\n",
    "                label: what you have named the labeled data (for us it is 'landcover')\n",
    "\n",
    "            Output: None ~ trains self.classifier\n",
    "        '''\n",
    "        \n",
    "        # prepare the training data\n",
    "        training = image.select(bands).sampleRegions(\\\n",
    "                   collection =  train_coll,\\\n",
    "                   properties = [label],\\\n",
    "                   scale = 30.0)            # I am currently unsure what this is for ~ leave as 30\n",
    "    \n",
    "        # Train the classifier\n",
    "        self.classifier = self.classifier.train(training, label, bands)\n",
    "    \n",
    "    \n",
    "    def get_training_accuracy(self):\n",
    "        '''\n",
    "            Input: None\n",
    "\n",
    "            Output: \n",
    "                float ~ the training accuracy of the classifier\n",
    "        '''\n",
    "\n",
    "        return self.classifier.confusionMatrix().accuracy().getInfo()\n",
    "    \n",
    "    def test(self, test_coll , image, bands, label):\n",
    "        '''\n",
    "            Input:\n",
    "                test_coll ~ featureCollection object with labeled data\n",
    "                image: image object you will test on\n",
    "                bands: list of strings ('B1' ect) the bands we are testing on - needs to be the same as train\n",
    "                label: what you have named the labeled data (for us it is 'landcover')\n",
    "\n",
    "            Output: sets the self.validated feature collection \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # test on data not trained on\n",
    "        validation = image.select(bands).sampleRegions(\\\n",
    "                   collection =  test_coll,\\\n",
    "                   properties = [label],\\\n",
    "                   scale = 30.0)            # I am currently unsure what this is for ~ leave as 30\n",
    "         \n",
    "\n",
    "        self.validated = validation.classify(self.classifier)    \n",
    "    \n",
    "    def get_testing_accuracy(self , label):\n",
    "        '''\n",
    "            Input:\n",
    "                label: what you have named the labeled data (for us it is 'landcover')\n",
    "            Output:\n",
    "                float ~ the training accuracy of the classifier\n",
    "        '''        \n",
    "        return self.validated.errorMatrix(label, 'classification').accuracy().getInfo()\n",
    "        \n",
    "    \n",
    "    def apply_to_image(self, image , bands):\n",
    "        # function to apply classifier to an entire image\n",
    "        return image.select(bands).classify(self.classifier)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image =  image_list_nyc[5]  # Randomly select the fifth image\n",
    "testing_image = image_list_nyc[0]\n",
    "\n",
    "b =  ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11']\n",
    "l = 'landcover'\n",
    "cls = Regression_Tree_Classifier()\n",
    "\n",
    "cls.set_classifier(50 , 1)\n",
    "cls.train(training_feature_collection, training_image , b, l)\n",
    "#cls.train(training_feature_collection, testing_image , b, l) # including this makes a big difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999975"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I am not sure why this takes longer than the actual training\n",
    "cls.get_training_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.c Test model with Image we Trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls.test(testing_feature_collection, training_image, b , l)\n",
    "cls.get_testing_accuracy(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/f43c7318ed29a15fd15b53abe6cc6b00-4f19ebbf7aa6736bc609990e68e272f0:getPixels\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = cls.apply_to_image(training_image , bands)\n",
    "parameters = {'min': 0.0,\n",
    "              'max': 1,\n",
    "              'dimensions': 768,\n",
    "              'palette': ['white', 'blue']}\n",
    "\n",
    "Image(url = result.clip(nycsite).getThumbUrl(parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.d Test model with different image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59175"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls.test(testing_feature_collection, testing_image, b , l)\n",
    "cls.get_testing_accuracy(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/c76498acb381374dc732716fd878ad8a-023ab4ddd407d4eef1e98c2d06759fac:getPixels\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = cls.apply_to_image(testing_image , bands)\n",
    "Image(url = result2.clip(nycsite).getThumbUrl(parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4. Takaways\n",
    "As of now, the model accurately fits the training data (probably overfits). It performs well when used on the image from with the training data came from. it does not extrapolate well to other locations. \n",
    "\n",
    "### ideas.  \n",
    "<dir>\n",
    "    1. Use a cloud mask ~ remove all pixels with clouds <br>\n",
    "    2. Train over all images <br>\n",
    "    3. Choose a model with higher capacity <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earth_analytics",
   "language": "python",
   "name": "earth_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
